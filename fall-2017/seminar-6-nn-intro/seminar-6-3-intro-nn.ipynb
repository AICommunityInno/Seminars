{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный материал основан на статье [A Quick Introduction to Neural Networks](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое ИНС"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Искусственная нейроная сеть** - это математическая модель, которая грубо приближает работу человеческого мозга.  \n",
    "Является одним из методов машинного обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нейрон"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нейрон - наименьшая единица в нейронной сети. Он представляет некоторую функцию $f$ (обычно нелинейную) от его входов. Каждый вход имеет свой вес $w$, так чтобы нейрон мог отдавать приоритет каким-то из входов больше, чем другим.\n",
    "\n",
    "![Нейрон](images/neuron.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем виде, нейрон делает следующее преобразование:\n",
    "$$y = f(\\sum_{i=1}^M{x_i \\cdot w_i} + b)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции $f$ обычно нелинейные, иначе модель свернулась бы к простой линейной регрессии. Несколько примеров нелинейных функций, которые обычно приментяются в нейронных сетях:\n",
    "\n",
    "$$\\text{1. Sigmoid: } \\sigma(x) = \\frac{1}{1+e^{-x}}$$  \n",
    "$$\\text{2. Hyperbolic tanh: } tanh(x)=\\frac{e^{-x} - e^x}{e^{-x} + e^x}$$  \n",
    "$$\\text{3. ReLU (Rectified Linear Unit): } f(x)=max\\{0, x\\}$$\n",
    "\n",
    "![Функции активации](images/activation_funcs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward сеть"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представляет собой прямую последовательность слоев из нейронов, связи между которыми не формируют цикл ([Википедия](https://en.wikipedia.org/wiki/Feedforward_neural_network))\n",
    "\n",
    "![Однослойная Feedforward сеть](images/ff_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Входной слой** представляет из себя значения по каждому признаку исследуемого объекта.  \n",
    "**Скрытые слои** содержат закодированную информацию из предыдущего слоя. Этих слоев может быть $\\geq 0$  \n",
    "**Выходной слой** является \"презентором\" закодированной сетью информацией. Например, может выдавать класс, к которому принадлежит объект в задаче классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Перцептрон* - нейрон, у которого функцией активации является [функция Хевисайда](https://ru.wikipedia.org/wiki/%D0%A4%D1%83%D0%BD%D0%BA%D1%86%D0%B8%D1%8F_%D0%A5%D0%B5%D0%B2%D0%B8%D1%81%D0%B0%D0%B9%D0%B4%D0%B0):\n",
    "\n",
    "$$f(x) = \\begin{cases} 1, & \\mbox{if } x \\geq 0 \\\\ 0, & \\mbox{if } x < 0 \\end{cases}$$\n",
    "\n",
    "*Однослойный перцептрон* - это нейронная сеть, содержащая $0$ скрытых слоев, *многослойный* содержит более $\\geq 1$ скрытых слоев."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обратное распространение ошибки (Backward Propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как учить-то сеть? Ответ - back-propagation.  \n",
    "**Backward Propagation** - самый популярный метод обучения нейронных сетей. Он подразумевает, что после прямого прохода данных через сеть (forward этап), часть информации, необходимая для обновления весов нейронов, сохраняется и используется при обратном проходе через сеть (backward этап).  \n",
    "В общем, алгоритм выглядит так:\n",
    "1. Проход данных через сеть в прямом направлении\n",
    "2. Полученный результат сравнивается с известными значениями из выборки.\n",
    "3. Ошибка распространяется обратно по сети, обновляя веса нейронов так. В следующий раз сеть \"должна\" ошибаться меньше.\n",
    "\n",
    "![Минимизация функции потерь](images/nn_cost.png)\n",
    "[Источник картинки](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обратное распространение ошибки на примере"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Оценки студентов](images/students_eval_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть выдала неправильный вывод на примере из первой строки таблицы:\n",
    "![Некорректный вывод сети](images/incorrect_nn_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошо, обновим веса согласно ошибке:\n",
    "![Обновление весов](images/weights_updating.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сеть более точна в своих предсказаниях:\n",
    "![Скорректированный выход сети](images/corrected_nn_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример вывода градиентов в back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Источник](https://www.ics.uci.edu/~pjsadows/notes.pdf)  \n",
    "Теперь давайте сделаем вывод градиентов на конктреном примере. Пусть нашей функцией ошибки будет функция *cross-entropy*:\n",
    "$$E = - \\sum_{i=1}^{nout}{t_i log(y_i) + (1-t_i)log(1-y_i)}$$\n",
    "\n",
    "Функцией активации в нейронах последнего слоя будет сигмоид:\n",
    "$$y_i = \\frac{1}{1+e^{-s_i}} \\text{, where } s_i = \\sum_{j}{h_j \\cdot w_{ji}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обновления весов между предпоследним и последним слоями нам нужно посчитать градиент функции ошибки по переменным весов между этими слоями:\n",
    "$$\\frac{\\partial E}{\\partial w_{ji}} = \\frac{\\partial E}{\\partial y_i} \\frac{\\partial y_i}{\\partial s_i} \\frac{\\partial s_i}{\\partial w_{ji}}$$\n",
    "\n",
    "Найдем требуемые производные:\n",
    "$$\\frac{\\partial E}{\\partial y_i} = \\frac{-t_i}{y_i} + \\frac{1-t_i}{1-y_i} = \\frac{y_i-t_i}{y_i(1-y_i)}$$\n",
    "$$\\frac{\\partial y_i}{\\partial s_i} = y_i(1-y_i)$$\n",
    "$$\\frac{\\partial s_i}{\\partial w_{ji}} = h_j$$\n",
    "\n",
    "Теперь, когда соберем все вместе:\n",
    "$$\\frac{\\partial E}{\\partial w_{ji}} = (y_i-t_i)h_j$$\n",
    "\n",
    "Все, мы получили градиенты весов между двумя последними слоями. Повторив то же самое для весов между 1-м и 2-м слоями и считая, что функцией активации во 2-м слое был так же сигмоид $h_j=\\frac{1}{1+e^{-s_j^1}}$, мы получим следующие градиенты весов между 1-м и 2-м слоями:\n",
    "$$\\frac{\\partial E}{\\partial w_{kj}^1} = \\frac{\\partial E}{\\partial s_{j}^1} \\frac{\\partial s_{j}^1}{\\partial w_{kj}} = \\sum_{i=1}^{nout}{(y_i-t_i)(w_{ji})(h_j(1-h_j))(x_k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример обучения двуслойного перцептрона на MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем [пример из библиотеки keras](https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "# Кол-во примеров, которые будут проходить через сеть при одном проходе\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# Загрузка и предобработка данных\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784)\n",
    "x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9] 3\n",
      "[ 0.  1.] [ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# Исходные данные представляют изображения и класс-цифра как метки.\n",
    "# Нам нужно преобразовать метки в бинарные векторы, т.к. мы будем использовать\n",
    "# функцию ошибки cross_entropy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(np.unique(y_train), y_train[7])\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(np.unique(y_train), y_train[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Создаем нашу сеть!\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "401920"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "784*512 + 512 # num_of_x * num_of_w + num_of_b_components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Устанавливаем функцию ошибки и метод оптимизации модели\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=RMSprop(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 4s - loss: 0.2186 - acc: 0.9331 - val_loss: 0.0997 - val_acc: 0.9692\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0825 - acc: 0.9752 - val_loss: 0.0848 - val_acc: 0.9736\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0551 - acc: 0.9829 - val_loss: 0.0986 - val_acc: 0.9709\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0375 - acc: 0.9880 - val_loss: 0.0816 - val_acc: 0.9783\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0291 - acc: 0.9910 - val_loss: 0.0923 - val_acc: 0.9786\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0227 - acc: 0.9926 - val_loss: 0.0794 - val_acc: 0.9820\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0196 - acc: 0.9941 - val_loss: 0.0872 - val_acc: 0.9806\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0162 - acc: 0.9950 - val_loss: 0.1023 - val_acc: 0.9801\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0137 - acc: 0.9957 - val_loss: 0.1180 - val_acc: 0.9794\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0121 - acc: 0.9965 - val_loss: 0.1057 - val_acc: 0.9814\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0107 - acc: 0.9969 - val_loss: 0.1152 - val_acc: 0.9812\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0100 - acc: 0.9971 - val_loss: 0.1014 - val_acc: 0.9838\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0089 - acc: 0.9973 - val_loss: 0.1361 - val_acc: 0.9790\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0085 - acc: 0.9974 - val_loss: 0.1190 - val_acc: 0.9825\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0080 - acc: 0.9979 - val_loss: 0.1333 - val_acc: 0.9820\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0061 - acc: 0.9983 - val_loss: 0.1509 - val_acc: 0.9804\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0069 - acc: 0.9982 - val_loss: 0.1633 - val_acc: 0.9782\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0065 - acc: 0.9984 - val_loss: 0.1344 - val_acc: 0.9820\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0053 - acc: 0.9988 - val_loss: 0.1500 - val_acc: 0.9810\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 2s - loss: 0.0055 - acc: 0.9988 - val_loss: 0.1437 - val_acc: 0.9830\n",
      "Test loss: 0.143663399831\n",
      "Test accuracy: 0.983\n"
     ]
    }
   ],
   "source": [
    "# Обучаем нашу модель и тестируем точность классификации цифр\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что дальше? Пробуйте [новые слои](https://keras.io/layers/core/), практикуйтесь на других простых примерах.  \n",
    "Посмотрите [другие функции активации](http://ruder.io/optimizing-gradient-descent/index.html#rmsprop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
