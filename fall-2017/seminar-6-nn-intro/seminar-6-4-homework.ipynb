{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Community @ Семинар №6. Введение в нейронные сети. Часть 1\n",
    "## Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеется двуслойная полносвязная нейронная сеть, классифицирующая объекты на $K$ классов. Ее вход представлен $N$ нейронами, а единственный скрытый слой состоит из $M$ нейронов.  \n",
    "![Задание 1. Нейронная сеть](images/hw_task1.png) \n",
    "\n",
    "Функции активации для скрытого и последнего слоя определены соответственно следующим образом:\n",
    "$$\\text{1) ReLU:} \\space h_m = max\\{0, s_m^{(1)}\\} \\text{, где } s_m^{(1)} = \\sum_{n}{w_{n,m}^{(1)}x_n}$$\n",
    "$$\\text{2) Sigmoid:} \\space y_k = \\frac{1}{1+e^{-s_k^{(2)}}} \\text{, где } s_k^{(2)} = \\sum_{m}{w_{m,k}^{(2)}h_m} $$\n",
    "\n",
    "Здесь, $w_{n,m}^{(1)}$ - принятое обозначение веса с индексами n, m между входным и скрытым слоями, а $w_{m,k}^{(2)}$ - обозначение веса с индексами m, k между скрытым и выходным слоями. $x_n - n$-й признак объекта, $h_m - $ результат $m$-го нейрона в скрытом слое. Также, $x_1=1$ и $h_1=1$ считаются фиктивными.\n",
    "\n",
    "В качестве функции ошибки используется cross-entropy:\n",
    "$$E = - \\sum_{k}{t_k \\log{y_k} + (1-t_k) \\log{(1-y_k)}}$$\n",
    "\n",
    "Сделайте back-propagation шаги (напишите градиенты всех весов) этой сети. Для справки можете использовать [материал по back-propagation](https://www.ics.uci.edu/~pjsadows/notes.pdf), упомянутый в лекции. Желательно представить результаты LaTeX'ом в этом ноутбуке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2 (*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объясните на картинке и выпишите аналитически, почему двуслойная нейронная сеть, состоящая только из полносвязных слоев с линейной функцией активации в скрытом слое, и на последнем слое имеющая функцию активации [softmax](https://github.com/Kulbear/deep-learning-nano-foundation/wiki/ReLU-and-Softmax-Activation-Functions#softmax), будет являться обычной логистической регрессией.  \n",
    "`Подсказка`: начните расписывать выражения с конца сети и используйте переобозначения для сумм, являющихся аргументами функций активаций."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
