{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    " # AI Community @ Семинар № 4\n",
    " ## Деревья решений  \n",
    " <img src=\"./data/CART_tree_titanic_survivors.png\" alt=\"Drawing\" style=\"height: 300px;\"/>\n",
    " ### Важная теория\n",
    " Энтропия.  \n",
    " Мера \"однорости\" данных  \n",
    " Низкая энтропия означает, что данные однородные  \n",
    " Высокая - что данные разнообразные  \n",
    " $$H(S)=\\sum_{x \\in X}{-p(x)\\log_2 p(x)}$$\n",
    " $S$ - данные  \n",
    " $X$ - множество возможных значений данных  \n",
    " $p(x)$ - частота $x$ в $S$\n",
    " \n",
    " Пример:  \n",
    " Пусть в данных 0 встретился 1 раз, 1 встретилась 99 раз. Тогда энтропия равна\n",
    " $$H(S) = - 0.01 \\cdot \\log_2{0.01} - 0.99 \\cdot \\log_2{0.99} = 0.0808$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.080793135895911181"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "entropy([0.01, 0.99], base=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain (рус. Выигрыш от ветвления, прирост информации)\n",
    "$$IG(A,S) = H(S) - \\sum_{t \\in T} p(t)H(t)$$  \n",
    "$T$ – множество подмножеств, полученных разбиением по признаку A  \n",
    "$p(t)$ – доля элементов с признаком $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Алгоритм ID3\n",
    " 1. Найти признак с наибольшим IG\n",
    " 2. Разделить множество объектов по этому признаку.\n",
    " 3. Повторить для каждого подмножества, если его энтропия не равна нулю  \n",
    "\n",
    "Почитать ещё: http://www.saedsayad.com/decision_tree.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Логистическая регрессия на деревьях"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берём рукописные цифры - 4 и 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "digits = datasets.load_digits()\n",
    "X = digits.data[(digits.target == 4) | (digits.target == 7)]\n",
    "y = digits.target[(digits.target == 4) | (digits.target == 7)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Делим на train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем два дерева высоты 1 - одно для верхней половины изображения, другое для второй"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "classifier1 = DecisionTreeClassifier(criterion='entropy', max_depth=1)\n",
    "classifier1.fit(X_train[:, :32], y_train)\n",
    "classifier2 = DecisionTreeClassifier(criterion='gini', max_depth=1)\n",
    "classifier2.fit(X_train[:, 32:], y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Точность деревьев по отдельности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.902777777778\n",
      "0.861111111111\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, classifier1.predict(X_test[:, :32])))\n",
    "print(accuracy_score(y_test, classifier2.predict(X_test[:, 32:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберем выходы с деревьев в лог. регрессию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "X_agg = np.vstack([classifier1.predict(X_train[:,:32]), classifier2.predict(X_train[:,32:])]).T\n",
    "lclassifier = LogisticRegression()\n",
    "lclassifier.fit(X_agg, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916666666667\n"
     ]
    }
   ],
   "source": [
    "c1_preds = classifier1.predict(X_test[:, :32])\n",
    "c2_preds = classifier2.predict(X_test[:, 32:])\n",
    "X_test_agg = np.vstack([c1_preds, c2_preds]).T\n",
    "preds = lclassifier.predict(X_test_agg)\n",
    "print(accuracy_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим веса деревьев:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.71998839,  0.56366117]])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lclassifier.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Бустинг\n",
    " \n",
    ">*Ну ты строишь дерево, грубо говоря, возможно я ошибаюсь в этой штуке, потом ты смотришь там среднюю ошибку или что-то такое, короче, ну смысл был в том, что ты берешь это значение...*\n",
    "<div style=\"text-align: right\"> Восточная мудрость </div>\n",
    "\n",
    "1. Строим простой классификатор\n",
    "2. Считаем его ошибку (с учётом весов элементов выборки)\n",
    "3. Находим вес классификатора (исходя из ошибки)\n",
    "4. Обновляем веса элементов выборки с учётом ошибки и веса классификатора\n",
    "5. Повторяем $n$ раз\n",
    "6. Строем взвешенный классификатор\n",
    "\n",
    "### AdaBoost\n",
    "Дано: $(x_{1},y_{1}),\\ldots,(x_{m},y_{m})$ , $x_{i} \\in X,\\, y_{i} \\in Y = \\{-1, +1\\}$\n",
    "\n",
    "$D_{1}(i) = \\frac{1}{m}, i=1,\\ldots,m.$\n",
    "\n",
    "Для $t = 1,\\ldots,T$:\n",
    "\n",
    "* Находим классификатор $h_{t} : X \\to \\{-1,+1\\}$; $h_{t} = \\arg \\min_{h_{j} \\in \\mathcal{H}} \\epsilon_{j}$, где $ \\epsilon_{j} = \\sum_{i=1}^{m} D_{t}(i)[y_i \\ne h_{j}(x_{i})]$\n",
    "* Если $\\epsilon_{t} \\geqslant 0.5$, то останавливаемся.\n",
    "* Выбираем $\\alpha_{t} \\in \\mathbf{R}$, обычно $\\alpha_{t}=\\cfrac{1}{2}\\textrm{ln}\\cfrac{1-\\epsilon_{t}}{\\epsilon_{t}}$ где $\\epsilon_{t}$ взвешенная ошибка классификатора $h_{t}$.\n",
    "* Обновляем:\n",
    "$D_{t+1}(i) = D_{t}(i) \\, e^{- \\alpha_{t} y_{i} h_{t}(x_{i})}$<br />\n",
    "* Нормализуем $D_{t+1}(i)$\n",
    "\n",
    "Строим результирующий классификатор:\n",
    "\n",
    "$H(x) = \\textrm{sign}\\left( \\sum_{t=1}^{T} \\alpha_{t}h_{t}(x)\\right)$\n",
    "\n",
    "Красивые картинки для линейной регрессии тут: http://cmp.felk.cvut.cz/~sochmj1/adaboost_talk.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
