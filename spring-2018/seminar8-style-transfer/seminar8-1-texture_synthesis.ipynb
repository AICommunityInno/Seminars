{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Community @ Семинар № 8, весна 2018\n",
    "## Texture synthesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед тем как приступать непосредственно к переносу стилей, научимся генерировать текстуры из данного шаблона.   \n",
    "Пусть у нас есть следующий фрагмент\n",
    "<img src='images/pebbles.jpg'>\n",
    "Как мы помним, каждый сверточный слой выделяет на изображении определенные низкоуровневые признаки. Прогоним данное изображение через глубокую сверточную сеть. При этом будем сохранять значения полученные после очередного слоя. Они будут иметь размерность $(N \\times C \\times H \\times W)$. Сейчас будем рассматривать один пример, т.е. размерность будет $(C \\times H \\times W)$.  \n",
    "N - число примеров(в данном случае всегда 1), C - количество карт признаков, H, W - ширина и высота изображения соответсвенно.  \n",
    "<img src='images/featuremap.png'>\n",
    "<div style=\"text-align: right\"><font size=\"2\">Изображение из презентации [2]</font></div>   \n",
    "Данный тензор можно представить в виде $H \\times W$ векторов размерности $C$.   \n",
    "Выберем среди них два произвольных вектора $a$ и $b$ и посчитаем для них матрицу Грэма\n",
    "([Вики](https://ru.wikipedia.org/wiki/%D0%9E%D0%BF%D1%80%D0%B5%D0%B4%D0%B5%D0%BB%D0%B8%D1%82%D0%B5%D0%BB%D1%8C_%D0%93%D1%80%D0%B0%D0%BC%D0%B0))   \n",
    "    \n",
    "$$G_{ij} = a_{i} * b_{j}$$\n",
    "\n",
    "Она будет приближенно отражать ковариацию между занчениями результатов каждого фильтра. Наша цель - получить изображение в том же распределении. Равенство ковариаций между результатами различных фильтров будет давать искомый эффект.    \n",
    "Почему именно матрица Грэма? Потому что, как обычно в глубоком обучении и происходит: попробовали -> заработало. Также, как мы увидим ниже, ее очень просто и эффективно считать.    \n",
    "Теперь посчитаем такие матрицы для всех возможных пар векторов и усредним полученные значения поделив на $H \\times W$. В результате мы получим $(C \\times C)$ матрицу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как считать ее на практике:   \n",
    "Обозначим за $F$ тензор признаков вытянутый в длину так, чтобы в результате размерность стала равна $(C \\times (H * W))$. Тогда   \n",
    "$$G = FF^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вспомним как тренируются нейронные сети. Мы считаем $\\frac{\\delta L}{\\delta w_{ij}^{(k)}}$, а затем вычитаем эти производные из $w_{ij}^{(k)}$. Таким образом мы минимизируем функцию ошибки $L$.  \n",
    "1. Возьмем претренированную сверточную нейронную сеть и зафиксируем ее, т.е. мы больше не собираемся в ней ничего менять. Прогоним оригинальное изображение и посчитаем матрицы Грэма $G_{ij}^l$ на каждом слое.    \n",
    "2. Сгенерируем изображение $I$ из случайного шума. Это изображение мы теперь будем улучшать, чтобы оно стало похожим на исходное изображение.\n",
    "3. Прогоним сгенерированное изображение через сеть, так же посчитаем матрицы Грэма $\\hat{G}_{ij}^l$ на каждом слое.   \n",
    "4. Определим функцию ошибки, как взвешенную сумму $L_2^2$ расстояний между матрицами Грэма оригинального изображения и соответствующими матрицами сгенерированного изображения.\n",
    "$$E_l = \\sum_{i,j} (G_{ij}^l - \\hat{G}_{ij}^l)^2$$\n",
    "$$L = \\sum_l w_l E_l$$, где $w_l$ - действтельные числа, веса, с которыми ошибка на каждом слое войдет в общую ошибку. Это гиперпараметры, т.е. задаются до тренировки.\n",
    "5. Посчитаем $\\frac{\\delta L}{\\delta I_{ij}}$, где $I_{ij}$ - пиксели изображения $I$. Обновим изображение $I_{ij} = I_{ij} - \\alpha \\frac{\\delta L}{\\delta I_{ij}}$ ($\\alpha$ - learning rate).    \n",
    "6.  Проделаем 3, 4, 5 пока не получим желаемую текстуру. Результат будет хранится в $I$.\n",
    "\n",
    "Все это проиллюстрировано и так же расписано на изображении ниже."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Замечания к изображению:\n",
    "1. Здесь формула ошибки изображена только на последнем слое, но лосс считается на нескольких слоях, которые мы сами выбираем. Интуиция следующая: слои в начале сети будут сохранять низкоуровневые признаки, такие как, напрмер, грани, а те, что ближе к концу, будут говорить о структуре изображения в целом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/texture_gen_scheme.png'>\n",
    "<div style=\"text-align: right\"><font size=\"2\">Изображение из статьи [1]</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ссылки:   \n",
    "1. [Статья на arxiv](https://arxiv.org/pdf/1505.07376.pdf)\n",
    "2. [Презентация cs231n](http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture12.pdf)\n",
    "3. [Реализация на ...](https://github.com/leongatys/DeepTextures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
