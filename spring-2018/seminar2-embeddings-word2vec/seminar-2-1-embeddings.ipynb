{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# AI Community @ Семинар № 2\n",
    "## Введение в Word Embeddings Models. CBOW и Skip-Gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Introduction to Word2Vec & How it Works](https://www.adityathakker.com/introduction-to-word2vec-how-it-works/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея представления слов в некотором абстрактном пространстве появилась в 1960-х. Она интересна тем, что позволяет придать словам **\"математическое значение смысла\"**. В самом начале использовались такие модели как Latent Semantic Analysis и Latent Dirichlet Allocation. В этом ноутбуке мы разберем более современные модели, основанные на нейронных сетях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пространство слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/one-hot.png)\n",
    "[Источник](https://medium.com/data-science-group-iitr/word-embedding-2d05d270b285)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В первую очередь необходимо представить слово вектором. **One-hot** представление - классический вариант. В нем 1 стоит на месте слова, присутствующего в текстовом примере и 0 в остальных компонентах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word embeddings** - плотные представления векторов-слов, осуществляющие трансформации из пространства высокой размерности one-hot векторов в пространство намного более низкой размерности. Например, если в словаре 50000 слов, то с помощью эмбеддингов можно представить слова векторами размерности 300 или даже 100. И при этом схожие слова в новом пространстве будут иметь схожие по некоторой метрике значения. В NLP часто используют косинусное расстояние между вектрами как метрику их схожести."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "История эмбеддингов слов началась с 2003 года, когда вышла статья [Neural Probabilistic Language Models (Bengio et al.)](https://link.springer.com/chapter/10.1007/3-540-33486-6_6). После этого в 2008 вышла статья [A unified architecture for Natural Language Processing (Ronan Collobert, Jason Weston)](https://ronan.collobert.com/pub/matos/2008_nlp_icml.pdf) и сделала эмбеддинги мэйнстримом. Последним серьезным прорывом стала статья 2013 года [Efficient Estimation of Word Representations in Vector Space (Tomas Mikolov)](https://arxiv.org/abs/1301.3781), которая вводит принципы построения моделей CBOW и Skip-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec** - это класс моделей эмбеддингов слов. Он послужил началом появления глубоких нейронных сетей в NLP. Хотя сами модели эмбеддингов и не являются глубокими, они часто используются как входные признаки для других сетей, в том числе с сетями, где число скрытых слоев >= 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слова, представленные в *латентном пространстве (Latent space - означает некоторое абстрактное пространство, построенное внутри модели)* обладают замечательными свойствами. Осуществляя операции над векторами в этом пространстве, мы можем получать следующие интересные результаты:  \n",
    "$$[king] - [man] + [woman] \\approx [queen]$$  \n",
    "$$[walking] - [swimming] + [swam] \\approx [walked]$$  \n",
    "$$[madrid] - [spain] + [france] \\approx [paris]$$  \n",
    "[Источник примеров](https://www.distilled.net/resources/a-beginners-guide-to-word2vec-aka-whats-the-opposite-of-canada/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/cbow_skipgram.png)\n",
    "Ссылка на [источник](https://stats.stackexchange.com/questions/280222/why-the-skip-gram-model-is-called-as-predicting-source-context-words-from-the-ta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идеи рассуждений и картинки взяты [отсюда](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В модели Skip Gram предсказываются слова-соседи определенного слова в окне фиксированного размера, т.е. для слова $w_t$ предсказываются слова $w_{t-n}$, $w_{t-n+1}$, ..., $w_{t-1}$, $w_{t+1}$, ..., $w_{t+n-1}$, $w_{t+n}$, где $n$ - размер окна."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/word_neighbors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку задача эмбеддинга заключается в представлении слова в пространстве меньшей размерности (популярная размерность 300), для этого отлично подходят автокодировщики. В классическом автокодировщике размерность входного вектора равна размерности выходного."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имея словарь из $M$ слов (обычно словарь содержит несколько десятков тысяч слов) мы можем предсказывать вероятность принадлежности окну каждого слова из словаря. Таким образом, входом и выходом алгоритма будут $M$-мерные векторы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Входной вектор - one-hot вектор. Выходным вектором может быть вектор вероятностей, полученный с помощью функции Softmax, которая будет последним преобразованием в нашей модели. Softmax выбирают для того чтобы сумма вероятностей слов со всего словаря была равна единице."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/skip_gram_nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Простейшей моделью может стать однослойный перцептрон, он же логистическая регрессия. Картинка выше объясняет архитектуру модели. Скрытый слой предстваляет из себя 300 нейронов, а операцию, которую он выполняет - это перемножение $M$-мерного вектора на матрицу размера $M \\times 300$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Самое интересное свойство word2vec в том, что матрица $M \\times 300$ состоит из представлений слов в новом \n",
    "латентном пространстве, в котором близость слов обусловлена семантической близостью."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/word_vec_space.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continous Bag of Words(CBOW) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель CBOW является в каком-то смысле зеркальным отражением модели Skip Gram. В модели CBOW предсказывается слово по известным соседям (контекст в окне фиксированного размера), т.е. для слова последовательности слов $w_{t-n}$, $w_{t-n+1}$, ..., $w_{t-1}$, $w_{t+1}$, ..., $w_{t+n-1}$, $w_{t+n}$ предсказывается слово $w_t$ в середине окна, где $n$ - размер окна."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](data/cbow_nn.png)\n",
    "[Источник](http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_II_The_Continuous_Bag-of-Words_Model.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отличием от модели Skip Gram является только то, что входной вектор в нейронной сети представлен суммой one-hot векторов слов из контекста, а именно:\n",
    "$$h = \\frac{1}{2n}W \\cdot \\sum_{i=t-n, i \\neq t}^{t+n}{x_i} \\text{,}$$\n",
    "где $h$ - выход скрытого слоя, $W$ - матрица весов скрытого слоя, $t$ - индекс слова, для которого осуществляется предсказание, $x_i$ - one-hot вектор $i$-го слова в окне."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Примеры реализаций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [fastText от Facebook](https://github.com/facebookresearch/fastText) с обученными моделями для 294 языков. Основан на skip-gram.\n",
    "1. [word2vec от Google](https://github.com/danielfrg/word2vec) с описанием [здесь](https://code.google.com/archive/p/word2vec/). Обучить можно либо на своих данных, либо взять предобученные. [Здесь](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/) ссылка на модель, обученная на 100 миллиардах слов из Google News, размер вектора 300. Также к нему есть короткая [инструкция](http://nbviewer.jupyter.org/github/danielfrg/word2vec/blob/master/examples/word2vec.ipynb)\n",
    "1. Библиотека [spaCy](https://spacy.io/) с обученными моделями внутри (на данный момент 7 языков, русского нет). Если хочется натренировать свою модель, можно использовать библиотеку [Gensim](https://radimrehurek.com/gensim/index.html)\n",
    "1. [GloVe](https://nlp.stanford.edu/projects/glove/) - модель эмбеддинга от Стэнфорда. Основана на уникальном подходе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полезные ссылки:\n",
    "1. [Word2Vec resources](http://mccormickml.com/2016/04/27/word2vec-resources/#original-papers--resources-from-google-team)\n",
    "1. [Презентация Mikolov Learning Representations of Text using Neural Networks](https://docs.google.com/file/d/0B7XkCwpI5KDYRWRnd1RzWXQ2TWc/edit)\n",
    "1. [Enriching Word Vectors with Subword Information](https://arxiv.org/abs/1607.04606)\n",
    "1. [A Beginner’s Guide to word2vec AKA What’s the Opposite of Canada?](https://www.distilled.net/resources/a-beginners-guide-to-word2vec-aka-whats-the-opposite-of-canada/)\n",
    "1. [Introduction to Word Embedding Models with Word2Vec](https://taylorwhitten.github.io/blog/word2vec)\n",
    "1. [Визуализация с помощью gensim](https://github.com/MiguelSteph/word2vec-with-gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Практика"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы запустить следующий код понадобится не меньше 8 Гб оперативной памяти (можно меньше, тогда скорость зависит от скорости вашего диска).\n",
    "Также понадобится скачать [модель](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit), обученную на 3М слов из Google News. Модель представлена [Chris McCormick](http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6375303531479181"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin/data', binary=True)\n",
    "model.similarity('france', 'spain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.word_vec('france').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('france', 1.0),\n",
       " ('spain', 0.6375303268432617),\n",
       " ('french', 0.6326056718826294),\n",
       " ('germany', 0.6314354538917542),\n",
       " ('europe', 0.6264256238937378),\n",
       " ('italy', 0.6257959008216858),\n",
       " ('england', 0.6120775938034058),\n",
       " ('european', 0.6074904799461365),\n",
       " ('belgium', 0.5972345471382141),\n",
       " ('usa', 0.5948354601860046)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_vector(model.word_vec('france'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('russia', 0.7199794054031372),\n",
       " ('paris', 0.6665446758270264),\n",
       " ('north_korea', 0.47550004720687866),\n",
       " ('tom_cruise', 0.44884952902793884),\n",
       " ('lohan', 0.44570255279541016),\n",
       " ('joel', 0.44360217452049255),\n",
       " ('lindsay_lohan', 0.4432262182235718),\n",
       " ('megan_fox', 0.43829047679901123),\n",
       " ('heidi', 0.43660295009613037),\n",
       " ('russians', 0.4354246258735657)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paris = model.wv['paris']\n",
    "france = model.wv['france']\n",
    "russia = model.wv['russia']\n",
    "\n",
    "maybe_moscow = paris - france + russia\n",
    "\n",
    "model.similar_by_vector(maybe_moscow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.518113374710083),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'king'], negative=['man'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('north_korea', 0.4717600345611572),\n",
       " ('tom_cruise', 0.449579656124115),\n",
       " ('lohan', 0.44875311851501465),\n",
       " ('joel', 0.4456343352794647),\n",
       " ('lindsay_lohan', 0.445479154586792),\n",
       " ('heidi', 0.4405139684677124),\n",
       " ('megan_fox', 0.43860718607902527),\n",
       " ('britney', 0.4297367036342621),\n",
       " ('russians', 0.4293155372142792),\n",
       " ('moscow', 0.4288122057914734)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['paris', 'russia'], negative=['france'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7664012230995352"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similarity('woman', 'man')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
