{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Community @ Семинар № 6\n",
    "## Оптимизация. Градиентный спуск: теория."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Оптимизация в контексте машинного обучения\n",
    "Разберем на примере.\n",
    "\n",
    "Задача: рассмотрим задачу классификации. \n",
    "$$X^\\ell = \\left\\{ (x_i, y_i) \\right\\}_{i=1}^\\ell, x_i \\in \\mathbb{R}^n, y_i \\in \\{+1, -1\\}$$\n",
    "\n",
    "Применим для решения _разделяющий классификатор_:\n",
    "\n",
    "$$ a(x, w) = \\text{sign}(g(x, w)) = [g(x, w) > 0] - [g(x, w) < 0], $$\n",
    "\n",
    "где $[P(x)] = \\begin{cases}\n",
    "1, & P(x)\\ \\text{истинно;} \\\\\n",
    "0, & \\text{иначе.}\n",
    "\\end{cases}$, $w \\in \\mathbb{R}^n.$\n",
    "\n",
    "__Определения__. $w$ — _параметры модели, веса_.  \n",
    "$g(x, w)$ — _разделяющая (дискриминантная) функция_.  \n",
    "$g(x, w) = 0$ — уравнение _разделяющей поверхности_.  \n",
    "\n",
    "__Определение__. _Отступ_ (margin) объекта $x_i$ представляет собой следующее выражение:\n",
    "$$ M_i(w) = g(x_i, w) y_i$$\n",
    "\n",
    "Физический смысл: $M_i(w) < 0 \\Leftrightarrow$ $a(x, w)$ ошибается на $x_i$.\n",
    "\n",
    "![Типы объектов по их отступу](images/margins.png)\n",
    "\n",
    "__Определение__. _Функцией потерь_ на объекте $(x, y)$ для разделяющего классификатора $a(x, w) = \\text{sign}(g(x, w))$ называется следующая функция:\n",
    "$$ \\mathcal{L}(w, x, y) = \\left[ M_i(w) < 0 \\right] = \\left[ g(x_i, w) y_i < 0 \\right]$$\n",
    "\n",
    "\n",
    "__Определение__. _Эмпирическим риском_ разделяющего классификатора $a(x, w) = \\text{sign}(g(x, w))$ на выборке $X^\\ell$ называется следующая функция:\n",
    "\n",
    "$$Q(w) = \\sum_{i=1}^\\ell \\mathcal{L}(w, x_i, y_i) = \\sum_{i=1}^\\ell \\left[ g(x_i, w) y_i < 0 \\right]$$\n",
    "\n",
    "Таким образом эмпирический риск подсчитывает количество ошибок, которые совершает классификатор на выборке. Его минимизация уменьшит количество ошибок. Другими словами — при минимизации эмпирического риска алгоритм будет обучаться. Таким образом, решение задачи классификации мы свели к _задаче оптимизации_:\n",
    "\n",
    "$$ Q(w) \\rightarrow \\min_w$$\n",
    "\n",
    "К сожалению, выбранная нами функция потерь не является дифференцируемой, что осложняет ее оптимизацию. В этом случае производят оптимизацию ее верхней оценки.\n",
    "\n",
    "__Определение__. _Верхней оценкой_ $\\tilde{Q}(w)$ функции $Q(w)$ называют функцию, удовлетворяющую следующему условию:\n",
    "\n",
    "$$ \\forall w \\in \\mathcal{R}^n: Q(w) \\leqslant \\tilde{Q}(w) $$\n",
    "\n",
    "На практике используют множество верхних оценок.\n",
    "\n",
    "![Различные функции потерь](images/losses.png)\n",
    "\n",
    "### Градиентный спуск\n",
    "\n",
    "Пусть мы выбрали функцию $L(M)$ (произвольную). \n",
    "\n",
    "Затем, для обучения требуется решить следующую оптимизационную задачу:\n",
    "$$ Q(w) = \\sum_{i=1}^\\ell L(M_i(w)) = \\sum_{i=1}^\\ell L(g(x_i, w) y_i) \\rightarrow \\min_{w}$$\n",
    "\n",
    "Для ее решения применяют градиентный спуск:\n",
    "\n",
    "$$ w^{(0)} := \\text{начальное приближение}$$\n",
    "$$ w^{(t+1)} := w^{(t)} - h \\cdot \\nabla Q(w^{(t)}) = w^{(t)} - h\\cdot\\sum_{i=1}^\\ell \\nabla L(M_i(w^{(t)}))$$\n",
    "$$ \\nabla Q(w) = \\left( \\frac{\\partial Q}{\\partial w_j}(w) \\right)_{j=1}^n (w \\in \\mathbb{R}^n)$$\n",
    "\n",
    "В этом алгоритме $h$ называют _темпом обучения_ (learning rate). Один проход метода по всей выборке называется _эпохой_.\n",
    "\n",
    "Заметим, что на каждом шаге используется сразу вся выборка. Это вычислительно затратно, поэтому на практике используют _стохастический_ градиентный спуск — вместо всей выборки шаг делается для одного объекта.\n",
    "\n",
    "$$ w^{(t+1)} := w^{(t)} - h \\cdot \\nabla L(M_i(w^{(t)})$$\n",
    "\n",
    "_Замечание_. В стохастическом градиентном спуске шаг и эпоха — это разные вещи. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Источники:  \n",
    "[1] Лекции Константина Воронцова по машинному обучению: http://www.machinelearning.ru/wiki/images/5/53/Voron-ML-Lin-SG.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
