{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный материал основан на статье [A Quick Introduction to Neural Networks](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Community @ Семинар № 6\n",
    "## Обратное распространение ошибки (Backward Propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы вывели формулу для одного нейрона, но что делать, если количество слоёв достаточно велико? Бесконечно считать производные правилом цепи? Ответ - back-propagation.  \n",
    "**Backward Propagation** - самый популярный метод обучения нейронных сетей. Он подразумевает, что после прямого прохода данных через сеть (forward этап), часть информации, необходимая для обновления весов нейронов, сохраняется и используется при обратном проходе через сеть (backward этап).  \n",
    "В общем, алгоритм выглядит так:\n",
    "1. Проход данных через сеть в прямом направлении\n",
    "2. Полученный результат сравнивается с известными значениями из выборки.\n",
    "3. Ошибка распространяется обратно по сети, обновляя веса нейронов так. В следующий раз сеть \"должна\" ошибаться меньше.\n",
    "\n",
    "![Минимизация функции потерь](images_old/nn_cost.png)\n",
    "[Источник картинки](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Обратное распространение ошибки на примере"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Оценки студентов](images_old/students_eval_table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сеть выдала неправильный вывод на примере из первой строки таблицы:\n",
    "![Некорректный вывод сети](images_old/incorrect_nn_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Хорошо, обновим веса согласно ошибке:\n",
    "![Обновление весов](images_old/weights_updating.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь сеть более точна в своих предсказаниях:\n",
    "![Скорректированный выход сети](images_old/corrected_nn_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример вывода градиентов в back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Источник](https://www.ics.uci.edu/~pjsadows/notes.pdf)  \n",
    "Теперь давайте сделаем вывод градиентов на конктреном примере. Пусть нашей функцией ошибки будет функция *cross-entropy*:\n",
    "$$E = - \\sum_{i=1}^{nout}{t_i log(y_i) + (1-t_i)log(1-y_i)}$$\n",
    "\n",
    "Функцией активации в нейронах последнего слоя будет сигмоид:\n",
    "$$y_i = \\frac{1}{1+e^{-s_i}} \\text{, where } s_i = \\sum_{j}{h_j \\cdot w_{ji}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обновления весов между предпоследним и последним слоями нам нужно посчитать градиент функции ошибки по переменным весов между этими слоями:\n",
    "$$\\frac{\\partial E}{\\partial w_{ji}} = \\frac{\\partial E}{\\partial y_i} \\frac{\\partial y_i}{\\partial s_i} \\frac{\\partial s_i}{\\partial w_{ji}}$$\n",
    "\n",
    "Найдем требуемые производные:\n",
    "$$\\frac{\\partial E}{\\partial y_i} = \\frac{-t_i}{y_i} + \\frac{1-t_i}{1-y_i} = \\frac{y_i-t_i}{y_i(1-y_i)}$$\n",
    "$$\\frac{\\partial y_i}{\\partial s_i} = y_i(1-y_i)$$\n",
    "$$\\frac{\\partial s_i}{\\partial w_{ji}} = h_j$$\n",
    "\n",
    "Теперь, когда соберем все вместе:\n",
    "$$\\frac{\\partial E}{\\partial w_{ji}} = (y_i-t_i)h_j$$\n",
    "\n",
    "Все, мы получили градиенты весов между двумя последними слоями. Повторив то же самое для весов между 1-м и 2-м слоями и считая, что функцией активации во 2-м слое был так же сигмоид $h_j=\\frac{1}{1+e^{-s_j^1}}$, мы получим следующие градиенты весов между 1-м и 2-м слоями:\n",
    "$$\\frac{\\partial E}{\\partial w_{kj}^1} = \\frac{\\partial E}{\\partial s_{j}^1} \\frac{\\partial s_{j}^1}{\\partial w_{kj}} = \\sum_{i=1}^{nout}{(y_i-t_i)(w_{ji})(h_j(1-h_j))(x_k)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример обучения двуслойного перцептрона на MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import nn\n",
    "from torch.optim import SGD\n",
    "from torchvision.transforms import Normalize\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# Кол-во примеров, которые будут проходить через сеть при одном проходе\n",
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка и предобработка данных\n",
    "\n",
    "# Загружаем датасеты для тренировки и теста\n",
    "MNIST('data', train=True, download=True)\n",
    "MNIST('data', train=False, download=True)\n",
    "\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        data = torch.load(data_path)\n",
    "        self.data = data[0].reshape(-1, 28 * 28).float() / 255\n",
    "        self.label = data[1]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.data[index]\n",
    "        label = self.label[index]\n",
    "        return image, label\n",
    "    \n",
    "train = MNISTDataset('data/MNIST/processed/training.pt')\n",
    "test = MNISTDataset('data/MNIST/processed/test.pt')\n",
    "\n",
    "# Исходные данные представляют изображения и класс-цифра как метки.\n",
    "print(f'Train data shape: {list(train.data.shape)},', f'Train labels shape: {list(train.label.shape)}')\n",
    "print(f'Test data shape: {list(test.data.shape)},', f'Test labels shape: {list(test.label.shape)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем нашу сеть!\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \"\"\"Simple NN with hidden layers [300, 100]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 300, bias=True)\n",
    "        self.fc2 = nn.Linear(300, 100, bias=True)\n",
    "        self.fc3 = nn.Linear(100, 10, bias=True)\n",
    "        \n",
    "        # Этот блок достаточно создать один раз, так как он не содержит обучаемых параметров\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.relu(self.fc1(x))\n",
    "        x2 = self.relu(self.fc2(x1))\n",
    "        x3 = self.fc3(x2)\n",
    "        return x3\n",
    "    \n",
    "model = Model().to(device)\n",
    "summary(model, (1, 28 * 28), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Устанавливаем функцию ошибки и метод оптимизации модели\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(model.parameters(), lr=0.03)\n",
    "\n",
    "# Вместо того, что бы самим разбивать данные на батчи, мы предоставим эту работу pytorch\n",
    "\n",
    "train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем нашу модель и тестируем точность классификации цифр\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model = model.train()\n",
    "    t = tqdm(train_loader, total=len(train_loader))\n",
    "    for data, label in t:\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        t.set_description_str(desc=f'Loss={float(loss.data):.3f}', refresh=False)\n",
    "        \n",
    "predicted_labels = []\n",
    "true_labels = []\n",
    "model = model.eval()\n",
    "for data, label in test_loader:\n",
    "    data = data.to(device)\n",
    "    \n",
    "    output = model(data)\n",
    "    predicted_labels.append(output.argmax(dim=1).cpu())\n",
    "    true_labels.append(label)\n",
    "    \n",
    "predicted_labels = torch.cat(predicted_labels)\n",
    "true_labels = torch.cat(true_labels)\n",
    "print(\"Test accuracy:\", (predicted_labels == true_labels).float().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что дальше? Пробуйте [новые слои](https://keras.io/layers/core/), практикуйтесь на других простых примерах.  \n",
    "Посмотрите [другие функции активации](http://ruder.io/optimizing-gradient-descent/index.html#rmsprop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
