{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Community @ Семинар  №3\n",
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© Текущий материал по большей части адаптирован из лекции Open Data Science по [линейным моделям и классификаторам](https://habrahabr.ru/company/ods/blog/323890)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Немного о типах задач"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задачи классификации\n",
    "$Y = \\{-1, +1\\}$ - классификация на 2 класса  \n",
    "$Y = \\{1,...,M\\}$ - на $M$ не пересекающихся классов  \n",
    "$Y = \\{0, 1\\}^M$ - на $M$ классов, которые могут пересекаться\n",
    "#### Задачи регрессии\n",
    "$Y = \\mathbb{R}$ или $Y = \\mathbb{R}^m$\n",
    "#### Задачи ранжирования\n",
    "$Y$ - конечное упорядоченное множество\n",
    "\n",
    "© (адаптировано из [лекции Воронцова К.В. (ШАД)](https://www.youtube.com/watch?v=qLBkB4sMztk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Метод линейной регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задача - необходимо предсказать некоторое интересующее нас значение (пример: цена квартиры) по определенным признакам объектов (пример: жилая площадь, наличие балкона, год постройки дома и т.д.), обладая такой информацией о других примерах."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Линейная регрессия** - один из самых простых методов машинного обучения. Его модель основана на линейной зависимости $y$ от значений признаков $x$. Отсюда и название метода.\n",
    "$$y = w_0 + \\sum_{i=1}^{m}{w_i*x_i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы сделать формулу более компактной, добавим фиктивную размерность $x_0$:\n",
    "$$y = \\sum_{i=0}^{m}{w_i*x_i} = w^Tx$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$x_i$ - $i$-ый признак в наших данных (например, площадь квартиры), $x_0$ - фиктивный признак, всегда равен $1$. Наблюдения-признаки описываются матрицей $X_{[nx(m+1)]}$, где по столбцам - наши признаки (плюс фиктивный), а по строкам - известные примеры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Представим, что наши данные описываются следующей линейной моделью:\n",
    "$$y = Xw + \\epsilon$$\n",
    "где $y \\in \\mathbb{R}^n$ целевая переменная, $w$ - вектор параметров (веса), $X$ - матрица наблюдений, $\\epsilon$ - случайная ошибка, которую мы не можем прогнозировать.\n",
    "Для каждого примера в отдельности модель будет выглядеть так:\n",
    "$$y_i = \\sum_{j=0}^{m}w_{j}X_{ij} + \\epsilon_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все, что нам нужно сделать, чтобы предсказывать наше целевое значение $y$, это получить вектор весов $w$. Чтобы это сделать, рассмотрим один из возможных методов, который называется **методом наименьших квадратов (МНК)**. Этот метод ***минимизирует*** среднеквадратичную ошибку между реальным значением зависимой переменной и прогнозом, выданным нашей моделью:\n",
    "$$L(X, y, w) = \\frac{1}{2n} * \\sum_{i=1}^{n}{(y_i - w^Tx_i)^2} = \\frac{1}{2n} \\lVert{y - Xw}\\rVert_{2}^{2}=\\frac{1}{2n} (y - Xw)^T(y - Xw)$$\n",
    "$L$ - функция ошибки (loss function). Она встречается в машинном обучении повсюду, и обычно наша цель - минимизировать эту функцию, т.е. найти такое значение параметра, при котором она принимает наименьшее значение."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для того чтобы посчитать минимальное значение нашей функции ошибки, достаточно приравнять ее градиент по вектору $w$ к $0$ и решить соответствующее уравнение относительно $w$. Градиент - это вектор частных производных по каждой из компонент вектора, по которому мы его считаем. В данном случае, это $w$.  \n",
    "Такое простое решение вытекает из того, что наша функция ошибки выпуклая, и ее минимум находится как раз в точке единственного экстремума:\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\frac{\\partial}{\\partial w} \\frac{1}{2n} (y^Ty - 2y^TXw + w^TX^TXw) = \\frac{1}{2n} (-2X^Ty + 2X^TXw)$$\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = 0 \\iff \\frac{1}{2n} (-2X^Ty + 2X^TXw) = 0$$\n",
    "$$\\iff -X^Ty + X^TXw = 0 \\iff X^TXw = X^Ty \\iff w = (X^TX)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы получили искомый вектор весов $w$:\n",
    "$$w = (X^TX)^{-1}X^Ty$$\n",
    "Теперь для того, чтобы предсказать целевое значение для нового примера данных $x_{new}$, мы просто подставляем найденный $w$ и $x_{new}$ в нашу модель линейной регрессии:\n",
    "$$y = w^Tx_{new}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
